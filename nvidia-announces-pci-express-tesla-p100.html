<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=description content="Kicking off this week in Frankfurt, Germany is the annual International Supercomputing Conference, better known as ISC. One of the two major supercomputing conferences for the year, ISC is commonly used as a backdrop for high performance processor announcements, and this year is no different. Starting things off this year is NVIDIA, who is taking"><meta name=author content="Jenniffer Sheldon"><meta name=generator content="Hugo 0.98.0"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=robots content="index,follow,noarchive"><link rel=stylesheet href=https://assets.cdnweb.info/hugo/base16/css/style.css type=text/css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700" type=text/css><link rel=alternate href=./index.xml type=application/rss+xml title=SwiftBlog><title>NVIDIA Announces PCI Express Tesla P100: Shipping In Q416 - SwiftBlog</title></head><body><header><div class="container clearfix"><a class=path href=./index.html>[SwiftBlog]</a>
<span class=caret># _</span><div class=right></div></div></header><div class=container><main role=main class=article><article class=single itemscope itemtype=http://schema.org/BlogPosting><div class=meta><span class=key>published on</span>
<span class=val><time itemprop=datePublished datetime=2024-07-29>July 29, 2024</time></span>
<span class=key>in</span>
<span class=val><a href=./categories/blog>blog</a></span></div><h1 class=headline itemprop=headline>NVIDIA Announces PCI Express Tesla P100: Shipping In Q416</h1><section class=body itemprop=articleBody><p>Kicking off this week in Frankfurt, Germany is the annual International Supercomputing Conference, better known as ISC. One of the two major supercomputing conferences for the year, ISC is commonly used as a backdrop for high performance processor announcements, and this year is no different. Starting things off this year is NVIDIA, who is taking to the show to announce the PCI Express version of the Tesla P100 accelerator.</p><p><a href=#>We were first introduced to Tesla P100 back in April of this year</a>, when NVIDIA announced it at their 2016 GPU Technology Conference. Based on NVIDIA’s new Pascal architecture and their 16nm GP100 GPU, Tesla P100 is a significant step up from the Tesla K/M series and their respective 28nm Kepler/Maxwell GPUs. Besides being a bigger-still GPU, P100 introduces a number of new features including larger caches, instruction level preemptive context switching, and double speed (packed) FP16 compute.</p><p>The initial version of the P100 announced at the time was NVIDIA’s highest performing version, a 300W board using NVIDIA’s new mezzanine connector, and shipping with 56 of 60 SMs enabled. The mezzanine connector marked a radical departure from traditional NVIDIA Tesla card designs, but also one that was necessary to facilitate NVIDIA’s high-speed point-to-point NVLink bus. However not every customer needs the features of NVLink or wants to build systems specifically for the mezzanine connector, and this is where the PCIe version of the card fleshes out the Tesla P100 lineup.</p><table align=center border=0 cellpadding=0 cellspacing=1 width=650><tbody readability=1><tr class=tgrey readability=2><td align=center colspan=7>NVIDIA Tesla Family Specification Comparison</td></tr><tr class=tlblue><td width=140>&nbsp;</td><td align=center valign=middle width=126>Tesla P100<br>(Mezzanine)</td><td align=center valign=middle width=126>Tesla P100<br>(16GB)</td><td align=center valign=middle width=126>Tesla P100<br>(12GB)</td><td align=center valign=middle width=126>Tesla M40</td></tr><tr><td class=tlgrey>Stream Processors</td><td align=center valign=middle>3584</td><td align=center valign=middle>3584</td><td align=center valign=middle>3584</td><td align=center valign=middle>3072</td></tr><tr><td class=tlgrey>Core Clock</td><td align=center valign=middle>1328MHz</td><td align=center valign=middle>?</td><td align=center valign=middle>?</td><td align=center valign=middle>948MHz</td></tr><tr><td class=tlgrey>Boost Clock(s)</td><td align=center valign=middle>1480MHz</td><td align=center valign=middle>1300MHz</td><td align=center valign=middle>1300MHz</td><td align=center valign=middle>1114MHz</td></tr><tr><td class=tlgrey>Memory Clock</td><td align=center valign=middle>1.4Gbps HBM2</td><td align=center valign=middle>1.4Gbps HBM2</td><td align=center valign=middle>1.4Gbps HBM2</td><td align=center valign=middle>6Gbps GDDR5</td></tr><tr><td class=tlgrey>Memory Bus Width</td><td align=center valign=middle>4096-bit</td><td align=center valign=middle>4096-bit</td><td align=center valign=middle>3072-bit</td><td align=center valign=middle>384-bit</td></tr><tr><td class=tlgrey>Memory Bandwidth</td><td align=center valign=middle>720GB/sec</td><td align=center valign=middle>720GB/sec</td><td align=center valign=middle>540GB/sec</td><td align=center valign=middle>288GB/sec</td></tr><tr><td class=tlgrey>VRAM</td><td align=center valign=middle>16GB</td><td align=center valign=middle>16GB</td><td align=center valign=middle>12GB</td><td align=center valign=middle>12GB</td></tr><tr><td class=tlgrey>L2 Cache</td><td align=center valign=middle>4MB</td><td align=center valign=middle>4MB</td><td align=center valign=middle>3MB</td><td align=center valign=middle>3MB</td></tr><tr><td class=tlgrey>Half Precision</td><td align=center valign=middle>21.2 TFLOPS</td><td align=center valign=middle>18.7 TFLOPS</td><td align=center valign=middle>18.7 TFLOPS</td><td align=center valign=middle>6.8 TFLOPS</td></tr><tr><td class=tlgrey>Single Precision</td><td align=center valign=middle>10.6 TFLOPS</td><td align=center valign=middle>9.3 TFLOPS</td><td align=center valign=middle>9.3 TFLOPS</td><td align=center valign=middle>6.8 TFLOPS</td></tr><tr><td class=tlgrey>Double Precision</td><td align=center valign=middle>5.3 TFLOPS<br>(1/2 rate)</td><td align=center valign=middle>4.7 TFLOPS<br>(1/2 rate)</td><td align=center valign=middle>4.7 TFLOPS<br>(1/2 rate)</td><td align=center valign=middle>213 GFLOPS<br>(1/32 rate)</td></tr><tr><td class=tlgrey>GPU</td><td align=center valign=middle>GP100</td><td align=center valign=middle>GP100</td><td align=center valign=middle>GP100</td><td align=center valign=middle>GM200</td></tr><tr><td class=tlgrey>Transistor Count</td><td align=center valign=middle>15.3B</td><td align=center valign=middle>15.3B</td><td align=center valign=middle>15.3B</td><td align=center valign=middle>8B</td></tr><tr><td class=tlgrey>TDP</td><td align=center valign=middle>300W</td><td align=center valign=middle>250W</td><td align=center valign=middle>250W</td><td align=center valign=middle>250W</td></tr><tr><td class=tlgrey>Form Factor</td><td align=center valign=middle>Mezzanine</td><td align=center valign=middle>PCIe</td><td align=center valign=middle>PCIe</td><td align=center valign=middle>PCIe</td></tr><tr><td class=tlgrey>Cooling</td><td align=center valign=middle>N/A</td><td align=center valign=middle>Passive</td><td align=center valign=middle>Passive</td><td align=center valign=middle>Passive</td></tr><tr><td class=tlgrey>Manufacturing Process</td><td align=center valign=middle>TSMC 16nm FinFET</td><td align=center valign=middle>TSMC 16nm FinFET</td><td align=center valign=middle>TSMC 16nm FinFET</td><td align=center valign=middle>TSMC 28nm</td></tr><tr><td class=tlgrey>Architecture</td><td align=center valign=middle>Pascal</td><td align=center valign=middle>Pascal</td><td align=center valign=middle>Pascal</td><td align=center valign=middle>Maxwell 2</td></tr></tbody></table><p>NVIDIA will be shipping two versions of the PCIe Tesla P100. The higher-end PCIe configuration is essentially a downclocked version of the original P100 on a PCIe card. In this case we’re looking at the same 56-of-60 SMs enabled, only with a boost clock of 1.3GHz rather than the original P100’s 1.48GHz. This puts theoretical throughput at 9.3 TFLOPs for FP32 and 4.7 TFLOPs for FP64, versus 10.6 TFLOPs and 5.3 TFLOPs respectively for the original P100. The change in clockspeed is to accommodate the lower TDP of the PCIe card; whereas the mezzanine cards are 300W, the PCIe cards are 250W, which is the same TDP as past generation Tesla PCIe cards. Shipping with the same TDP means that these PCIe cards can be used as drop-in replacements for older Tesla cards, since they have the same power and cooling requirements.</p><p>Meanwhile on the memory side of matters, the higher-end card ships with the full 16GB of HBM2 enabled. Clockspeeds haven’t been dialed back here at all, so it’s still 1.4Gbps HBM2 in a quad package configuration, allowing for 720GB/sec of bandwidth (both with and without ECC).</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/10433/P100Cards_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p>It’s on this latter point that the lower-end version of the PCIe Tesla P100 further changes things. The lower-end card ships with the same GPU clockspeeds and overall compute throughput, but it cuts the amount of memory and the memory bandwidth by 25%. This brings the total memory capacity down to 12GB, and the total memory bandwidth down to 540GB/sec. The L2 cache, which is directly tied to the memory controllers, is also reduced from 4MB to 3MB. NVIDIA has previously offered multiple tiers/prices of high-end Tesla cards – though usually under different model numbers to make them easier to differentiate – so having multiple PCIe cards is not unusual for the company.</p><p>Not explicitly said by the company (but is clear from the specifications) is that this is meant to be a salvage part for GP100. Because of the level of integration required by HBM2 memory, GP100 packages have to be fully assembled with their interposer and HBM2 ahead of time. This means that any problems with the package are permanent, and NVIDIA has to either toss or salvage the package. The lower-end PCIe card gives them the option of the latter; if a package comes out with a faulty HBM2 stack, interposer link, or HBM2 memory controller, then NVIDIA can disable the bad HBM2 stack and sell it rather than tossing it entirely.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/10433/P100Board2_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p>Both of these cards are going to be targeted at customers who either don’t need NVLink, or need drop-in card upgrades for current Tesla cards. The lack of NVLink will impact performance to some extent in multi-card systems, but it’s going to be heavily dependent on the workload. For workloads that don’t require a lot of high-speed communication between GPUs, then the impact will be minimal, which would make the PCIe version a good, conventional fit for those customers.</p><p>Along with releasing the specifications, NVIDIA has announced that the PCIe Tesla P100 will be available in Q4 of this year. Given the additional hardware required to house the original mezzanine version of the P100 and the fact that NVIDIA uses those boards for their own DGX-1 server box, I suspect we’re going to see that the PCIe Tesla P100 will be the first P100 available in non-NVIDIA systems. Do note however that pricing for the PCIe cards has yet to be announced.</p><p>Finally, buried in the PCIe Tesla P100 announcement, NVIDIA has also reconfirmed that the <a href=#>Piz Daint supercomputer</a> upgrade project is on schedule for later this year. The Swiss National Supercomputing Center will be doing a drop-in upgrade, replacing the supercomputer’s 4,500 Tesla K20X cards with Tesla P100 PCIe cards. This will be, to our knowledge, the first Pascal P100 based supercomputer to come online once the upgrade is completed.</p><h2>Software Updates: DIGITS 4, cnDNN 5.1, & GIE</h2><p>Along with the PCIe Tesla P100 announcement – though not strictly related to it – NVIDIA is also announcing some software updates to components of their Deep Learning SDK, the company’s collection of various software libraries and tools.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/10433/23_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p>Set to arrive in the near future, both cuDNN and DIGITS are receiving upgrades. Version 5.1 of cuDNN is a minor update to deliver performance improvements for ResNet style networks. Meanwhile DIGITS version 4 is more significant, with NVIDIA adding object detection/recognition functionality to their neutral network training system.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/10433/25_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/10433/24_575px.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p>DIGITS 4 is also specifically designed to go with NVIDIA’s previously revealed GPU Inference Engine (GIE) software package, which was announced back at GTC 2016. As NVIDIA extends their efforts to get into deep learning/neural networks, DIGITS’ object detection functionality aligns with NVIDA’s other efforts, allowing developers to actually use (run inference with) their DIGITS-powered neural networks. The use cases for Drive PX2 and the Jetson TX1 board are very much rooted in real-world semi-autonomous devices, while NVIDIA expects object detection to be a big deal for Tesla M4 customers who are doing video analysis.</p><p class=postsid style=color:rgba(255,0,0,0)>ncG1vNJzZmivp6x7orrAp5utnZOde6S7zGiqoaenZH5xgJJsZqeumZm2onnAp6WorZ6YsrR5z5ygZp2opb%2Bmv9Jmq56rnJZ6sX2PaQ%3D%3D</p></section></article></main></div><footer><div class=container><span class=copyright>&copy; 2024 SwiftBlog - <a rel=license href=http://creativecommons.org/licenses/by/4.0/>CC BY 4.0</a></span></div></footer><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://js.zainuddin.my.id/banner.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://js.zainuddin.my.id/tracking_server_6.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script>var _paq=window._paq=window._paq||[];_paq.push(["trackPageView"]),_paq.push(["enableLinkTracking"]),function(){e="//analytics.cdnweb.info/",_paq.push(["setTrackerUrl",e+"matomo.php"]),_paq.push(["setSiteId","1"]);var e,n=document,t=n.createElement("script"),s=n.getElementsByTagName("script")[0];t.async=!0,t.src=e+"matomo.js",s.parentNode.insertBefore(t,s)}()</script></body></html>